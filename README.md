# Transformer from Scratch
This repository contains a PyTorch implementation of the original Transformer architecture as described in the paper "Attention is All You Need" by Vaswani et al. This implementation focuses solely on the encoder-decoder architecture, excluding any training loops and data preprocessing (at least for now).
